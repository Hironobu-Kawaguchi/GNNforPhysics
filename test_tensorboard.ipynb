{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d628e2b2",
   "metadata": {},
   "source": [
    "# Tensorboard test ç”¨ notebook\n",
    "https://www.tensorflow.org/tensorboard/get_started?hl=ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301dfd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b54df0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "067eced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb38bdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 02:29:32.100266: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow will not use sklearn by default. This improves performance in some cases. To enable sklearn export the environment variable  TF_ALLOW_IOLIBS=1.\n",
      "WARNING:tensorflow:TensorFlow will not use Dask by default. This improves performance in some cases. To enable Dask export the environment variable  TF_ALLOW_IOLIBS=1.\n",
      "WARNING:tensorflow:TensorFlow will not use Pandas by default. This improves performance in some cases. To enable Pandas export the environment variable  TF_ALLOW_IOLIBS=1.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tree\n",
    "\n",
    "\n",
    "from learning_to_simulate import learned_simulator\n",
    "from learning_to_simulate import noise_utils\n",
    "from learning_to_simulate import reading_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c96a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# writer = tf.summary.FileWriter(log_dir)\n",
    "# # train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "# # test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "# # train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "# # test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bded62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_enum(\n",
    "    'mode', 'train', ['train', 'eval', 'eval_rollout'],\n",
    "    help='Train model, one step evaluation or rollout evaluation.')\n",
    "flags.DEFINE_enum('eval_split', 'test', ['train', 'valid', 'test'],\n",
    "                  help='Split to use when running evaluation.')\n",
    "flags.DEFINE_string('data_path', None, help='The dataset directory.')\n",
    "flags.DEFINE_integer('batch_size', 2, help='The batch size.')\n",
    "flags.DEFINE_integer('num_steps', int(2e7), help='Number of steps of training.')\n",
    "flags.DEFINE_float('noise_std', 6.7e-4, help='The std deviation of the noise.')\n",
    "flags.DEFINE_string('model_path', None,\n",
    "                    help=('The path for saving checkpoints of the model. '\n",
    "                          'Defaults to a temporary directory.'))\n",
    "flags.DEFINE_string('output_path', None,\n",
    "                    help='The path for saving outputs (e.g. rollouts).')\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "Stats = collections.namedtuple('Stats', ['mean', 'std'])\n",
    "\n",
    "INPUT_SEQUENCE_LENGTH = 6  # So we can calculate the last 5 velocities.\n",
    "NUM_PARTICLE_TYPES = 9\n",
    "KINEMATIC_PARTICLE_ID = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9df90419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kinematic_mask(particle_types):\n",
    "  \"\"\"Returns a boolean mask, set to true for kinematic (obstacle) particles.\"\"\"\n",
    "  return tf.equal(particle_types, KINEMATIC_PARTICLE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d654cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(tensor_dict):\n",
    "  \"\"\"Prepares a single stack of inputs by calculating inputs and targets.\n",
    "\n",
    "  Computes n_particles_per_example, which is a tensor that contains information\n",
    "  about how to partition the axis - i.e. which nodes belong to which graph.\n",
    "\n",
    "  Adds a batch axis to `n_particles_per_example` and `step_context` so they can\n",
    "  later be batched using `batch_concat`. This batch will be the same as if the\n",
    "  elements had been batched via stacking.\n",
    "\n",
    "  Note that all other tensors have a variable size particle axis,\n",
    "  and in this case they will simply be concatenated along that\n",
    "  axis.\n",
    "\n",
    "\n",
    "\n",
    "  Args:\n",
    "    tensor_dict: A dict of tensors containing positions, and step context (\n",
    "    if available).\n",
    "\n",
    "  Returns:\n",
    "    A tuple of input features and target positions.\n",
    "\n",
    "  \"\"\"\n",
    "  # Position is encoded as [sequence_length, num_particles, dim] but the model\n",
    "  # expects [num_particles, sequence_length, dim].\n",
    "  pos = tensor_dict['position']\n",
    "  pos = tf.transpose(pos, perm=[1, 0, 2])\n",
    "\n",
    "  # The target position is the final step of the stack of positions.\n",
    "  target_position = pos[:, -1]\n",
    "\n",
    "  # Remove the target from the input.\n",
    "  tensor_dict['position'] = pos[:, :-1]\n",
    "\n",
    "  # Compute the number of particles per example.\n",
    "  num_particles = tf.shape(pos)[0]\n",
    "  # Add an extra dimension for stacking via concat.\n",
    "  tensor_dict['n_particles_per_example'] = num_particles[tf.newaxis]\n",
    "\n",
    "  if 'step_context' in tensor_dict:\n",
    "    # Take the input global context. We have a stack of global contexts,\n",
    "    # and we take the penultimate since the final is the target.\n",
    "    tensor_dict['step_context'] = tensor_dict['step_context'][-2]\n",
    "    # Add an extra dimension for stacking via concat.\n",
    "    tensor_dict['step_context'] = tensor_dict['step_context'][tf.newaxis]\n",
    "  return tensor_dict, target_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc54f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rollout_inputs(context, features):\n",
    "  \"\"\"Prepares an inputs trajectory for rollout.\"\"\"\n",
    "  out_dict = {**context}\n",
    "  # Position is encoded as [sequence_length, num_particles, dim] but the model\n",
    "  # expects [num_particles, sequence_length, dim].\n",
    "  pos = tf.transpose(features['position'], [1, 0, 2])\n",
    "  # The target position is the final step of the stack of positions.\n",
    "  target_position = pos[:, -1]\n",
    "  # Remove the target from the input.\n",
    "  out_dict['position'] = pos[:, :-1]\n",
    "  # Compute the number of nodes\n",
    "  out_dict['n_particles_per_example'] = [tf.shape(pos)[0]]\n",
    "  if 'step_context' in features:\n",
    "    out_dict['step_context'] = features['step_context']\n",
    "  out_dict['is_trajectory'] = tf.constant([True], tf.bool)\n",
    "  return out_dict, target_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0399234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_concat(dataset, batch_size):\n",
    "  \"\"\"We implement batching as concatenating on the leading axis.\"\"\"\n",
    "\n",
    "  # We create a dataset of datasets of length batch_size.\n",
    "  windowed_ds = dataset.window(batch_size)\n",
    "\n",
    "  # The plan is then to reduce every nested dataset by concatenating. We can\n",
    "  # do this using tf.data.Dataset.reduce. This requires an initial state, and\n",
    "  # then incrementally reduces by running through the dataset\n",
    "\n",
    "  # Get initial state. In this case this will be empty tensors of the\n",
    "  # correct shape.\n",
    "  initial_state = tree.map_structure(\n",
    "      lambda spec: tf.zeros(  # pylint: disable=g-long-lambda\n",
    "          shape=[0] + spec.shape.as_list()[1:], dtype=spec.dtype),\n",
    "      dataset.element_spec)\n",
    "\n",
    "  # We run through the nest and concatenate each entry with the previous state.\n",
    "  def reduce_window(initial_state, ds):\n",
    "    return ds.reduce(initial_state, lambda x, y: tf.concat([x, y], axis=0))\n",
    "\n",
    "  return windowed_ds.map(\n",
    "      lambda *x: tree.map_structure(reduce_window, initial_state, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fbe98bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_fn(data_path, batch_size, mode, split):\n",
    "  \"\"\"Gets the learning simulation input function for tf.estimator.Estimator.\n",
    "\n",
    "  Args:\n",
    "    data_path: the path to the dataset directory.\n",
    "    batch_size: the number of graphs in a batch.\n",
    "    mode: either 'one_step_train', 'one_step' or 'rollout'\n",
    "    split: either 'train', 'valid' or 'test.\n",
    "\n",
    "  Returns:\n",
    "    The input function for the learning simulation model.\n",
    "  \"\"\"\n",
    "  def input_fn():\n",
    "    \"\"\"Input function for learning simulation.\"\"\"\n",
    "    # Loads the metadata of the dataset.\n",
    "    metadata = _read_metadata(data_path)\n",
    "    # Create a tf.data.Dataset from the TFRecord.\n",
    "    ds = tf.data.TFRecordDataset([os.path.join(data_path, f'{split}.tfrecord')])\n",
    "    ds = ds.map(functools.partial(\n",
    "        reading_utils.parse_serialized_simulation_example, metadata=metadata))\n",
    "    if mode.startswith('one_step'):\n",
    "      # Splits an entire trajectory into chunks of 7 steps.\n",
    "      # Previous 5 velocities, current velocity and target.\n",
    "      split_with_window = functools.partial(\n",
    "          reading_utils.split_trajectory,\n",
    "          window_length=INPUT_SEQUENCE_LENGTH + 1)\n",
    "      ds = ds.flat_map(split_with_window)\n",
    "      # Splits a chunk into input steps and target steps\n",
    "      ds = ds.map(prepare_inputs)\n",
    "      # If in train mode, repeat dataset forever and shuffle.\n",
    "      if mode == 'one_step_train':\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.shuffle(512)\n",
    "      # Custom batching on the leading axis.\n",
    "      ds = batch_concat(ds, batch_size)\n",
    "    elif mode == 'rollout':\n",
    "      # Rollout evaluation only available for batch size 1\n",
    "      assert batch_size == 1\n",
    "      ds = ds.map(prepare_rollout_inputs)\n",
    "    else:\n",
    "      raise ValueError(f'mode: {mode} not recognized')\n",
    "    return ds\n",
    "\n",
    "  return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57f90bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(simulator, features, num_steps):\n",
    "  \"\"\"Rolls out a trajectory by applying the model in sequence.\"\"\"\n",
    "  initial_positions = features['position'][:, 0:INPUT_SEQUENCE_LENGTH]\n",
    "  ground_truth_positions = features['position'][:, INPUT_SEQUENCE_LENGTH:]\n",
    "  global_context = features.get('step_context')\n",
    "  def step_fn(step, current_positions, predictions):\n",
    "\n",
    "    if global_context is None:\n",
    "      global_context_step = None\n",
    "    else:\n",
    "      global_context_step = global_context[\n",
    "          step + INPUT_SEQUENCE_LENGTH - 1][tf.newaxis]\n",
    "\n",
    "    next_position = simulator(\n",
    "        current_positions,\n",
    "        n_particles_per_example=features['n_particles_per_example'],\n",
    "        particle_types=features['particle_type'],\n",
    "        global_context=global_context_step)\n",
    "\n",
    "    # Update kinematic particles from prescribed trajectory.\n",
    "    kinematic_mask = get_kinematic_mask(features['particle_type'])\n",
    "    next_position_ground_truth = ground_truth_positions[:, step]\n",
    "    next_position = tf.where(kinematic_mask, next_position_ground_truth,\n",
    "                             next_position)\n",
    "    updated_predictions = predictions.write(step, next_position)\n",
    "\n",
    "    # Shift `current_positions`, removing the oldest position in the sequence\n",
    "    # and appending the next position at the end.\n",
    "    next_positions = tf.concat([current_positions[:, 1:],\n",
    "                                next_position[:, tf.newaxis]], axis=1)\n",
    "\n",
    "    return (step + 1, next_positions, updated_predictions)\n",
    "\n",
    "  predictions = tf.TensorArray(size=num_steps, dtype=tf.float32)\n",
    "  _, _, predictions = tf.while_loop(\n",
    "      cond=lambda step, state, prediction: tf.less(step, num_steps),\n",
    "      body=step_fn,\n",
    "      loop_vars=(0, initial_positions, predictions),\n",
    "      back_prop=False,\n",
    "      parallel_iterations=1)\n",
    "\n",
    "  output_dict = {\n",
    "      'initial_positions': tf.transpose(initial_positions, [1, 0, 2]),\n",
    "      'predicted_rollout': predictions.stack(),\n",
    "      'ground_truth_rollout': tf.transpose(ground_truth_positions, [1, 0, 2]),\n",
    "      'particle_types': features['particle_type'],\n",
    "  }\n",
    "\n",
    "  if global_context is not None:\n",
    "    output_dict['global_context'] = global_context\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "953712ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _combine_std(std_x, std_y):\n",
    "  return np.sqrt(std_x**2 + std_y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b63eb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_simulator(model_kwargs, metadata, acc_noise_std, vel_noise_std):\n",
    "  \"\"\"Instantiates the simulator.\"\"\"\n",
    "  # Cast statistics to numpy so they are arrays when entering the model.\n",
    "  cast = lambda v: np.array(v, dtype=np.float32)\n",
    "  acceleration_stats = Stats(\n",
    "      cast(metadata['acc_mean']),\n",
    "      _combine_std(cast(metadata['acc_std']), acc_noise_std))\n",
    "  velocity_stats = Stats(\n",
    "      cast(metadata['vel_mean']),\n",
    "      _combine_std(cast(metadata['vel_std']), vel_noise_std))\n",
    "  normalization_stats = {'acceleration': acceleration_stats,\n",
    "                         'velocity': velocity_stats}\n",
    "  if 'context_mean' in metadata:\n",
    "    context_stats = Stats(\n",
    "        cast(metadata['context_mean']), cast(metadata['context_std']))\n",
    "    normalization_stats['context'] = context_stats\n",
    "\n",
    "  simulator = learned_simulator.LearnedSimulator(\n",
    "      num_dimensions=metadata['dim'],\n",
    "      connectivity_radius=metadata['default_connectivity_radius'],\n",
    "      graph_network_kwargs=model_kwargs,\n",
    "      boundaries=metadata['bounds'],\n",
    "      num_particle_types=NUM_PARTICLE_TYPES,\n",
    "      normalization_stats=normalization_stats,\n",
    "      particle_type_embedding_size=16)\n",
    "  return simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3fb2486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_step_estimator_fn(data_path,\n",
    "                              noise_std,\n",
    "                              latent_size=128,\n",
    "                              hidden_size=128,\n",
    "                              hidden_layers=2,\n",
    "                              message_passing_steps=10):\n",
    "  \"\"\"Gets one step model for training simulation.\"\"\"\n",
    "  metadata = _read_metadata(data_path)\n",
    "\n",
    "  model_kwargs = dict(\n",
    "      latent_size=latent_size,\n",
    "      mlp_hidden_size=hidden_size,\n",
    "      mlp_num_hidden_layers=hidden_layers,\n",
    "      num_message_passing_steps=message_passing_steps)\n",
    "\n",
    "  def estimator_fn(features, labels, mode):\n",
    "    target_next_position = labels\n",
    "    simulator = _get_simulator(model_kwargs, metadata,\n",
    "                               vel_noise_std=noise_std,\n",
    "                               acc_noise_std=noise_std)\n",
    "    # Sample the noise to add to the inputs to the model during training.\n",
    "    sampled_noise = noise_utils.get_random_walk_noise_for_position_sequence(\n",
    "        features['position'], noise_std_last_step=noise_std)\n",
    "    non_kinematic_mask = tf.logical_not(\n",
    "        get_kinematic_mask(features['particle_type']))\n",
    "    noise_mask = tf.cast(\n",
    "        non_kinematic_mask, sampled_noise.dtype)[:, tf.newaxis, tf.newaxis]\n",
    "    sampled_noise *= noise_mask\n",
    "\n",
    "    # Get the predictions and target accelerations.\n",
    "    pred_target = simulator.get_predicted_and_target_normalized_accelerations(\n",
    "        next_position=target_next_position,\n",
    "        position_sequence=features['position'],\n",
    "        position_sequence_noise=sampled_noise,\n",
    "        n_particles_per_example=features['n_particles_per_example'],\n",
    "        particle_types=features['particle_type'],\n",
    "        global_context=features.get('step_context'))\n",
    "    pred_acceleration, target_acceleration = pred_target\n",
    "\n",
    "    # Calculate the loss and mask out loss on kinematic particles/\n",
    "    loss = (pred_acceleration - target_acceleration)**2\n",
    "\n",
    "    num_non_kinematic = tf.reduce_sum(\n",
    "        tf.cast(non_kinematic_mask, tf.float32))\n",
    "    loss = tf.where(non_kinematic_mask, loss, tf.zeros_like(loss))\n",
    "    loss = tf.reduce_sum(loss) / tf.reduce_sum(num_non_kinematic)\n",
    "    global_step = tf.train.get_global_step()\n",
    "    # Set learning rate to decay from 1e-4 to 1e-6 exponentially.\n",
    "    min_lr = 1e-6\n",
    "    lr = tf.train.exponential_decay(learning_rate=1e-4 - min_lr,\n",
    "                                    global_step=global_step,\n",
    "                                    decay_steps=int(5e6),\n",
    "                                    decay_rate=0.1) + min_lr\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    train_op = opt.minimize(loss, global_step)\n",
    "\n",
    "    # Calculate next position and add some additional eval metrics (only eval).\n",
    "    predicted_next_position = simulator(\n",
    "        position_sequence=features['position'],\n",
    "        n_particles_per_example=features['n_particles_per_example'],\n",
    "        particle_types=features['particle_type'],\n",
    "        global_context=features.get('step_context'))\n",
    "\n",
    "    predictions = {'predicted_next_position': predicted_next_position}\n",
    "\n",
    "    eval_metrics_ops = {\n",
    "        'loss_mse': tf.metrics.mean_squared_error(\n",
    "            pred_acceleration, target_acceleration),\n",
    "        'one_step_position_mse': tf.metrics.mean_squared_error(\n",
    "            predicted_next_position, target_next_position)\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        train_op=train_op,\n",
    "        loss=loss,\n",
    "        predictions=predictions,\n",
    "        eval_metric_ops=eval_metrics_ops)\n",
    "\n",
    "  return estimator_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d1eeb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rollout_estimator_fn(data_path,\n",
    "                             noise_std,\n",
    "                             latent_size=128,\n",
    "                             hidden_size=128,\n",
    "                             hidden_layers=2,\n",
    "                             message_passing_steps=10):\n",
    "  \"\"\"Gets the model function for tf.estimator.Estimator.\"\"\"\n",
    "  metadata = _read_metadata(data_path)\n",
    "\n",
    "  model_kwargs = dict(\n",
    "      latent_size=latent_size,\n",
    "      mlp_hidden_size=hidden_size,\n",
    "      mlp_num_hidden_layers=hidden_layers,\n",
    "      num_message_passing_steps=message_passing_steps)\n",
    "\n",
    "  def estimator_fn(features, labels, mode):\n",
    "    del labels  # Labels to conform to estimator spec.\n",
    "    simulator = _get_simulator(model_kwargs, metadata,\n",
    "                               acc_noise_std=noise_std,\n",
    "                               vel_noise_std=noise_std)\n",
    "\n",
    "    num_steps = metadata['sequence_length'] - INPUT_SEQUENCE_LENGTH\n",
    "    rollout_op = rollout(simulator, features, num_steps=num_steps)\n",
    "    squared_error = (rollout_op['predicted_rollout'] -\n",
    "                     rollout_op['ground_truth_rollout']) ** 2\n",
    "    loss = tf.reduce_mean(squared_error)\n",
    "    eval_ops = {'rollout_error_mse': tf.metrics.mean_squared_error(\n",
    "        rollout_op['predicted_rollout'], rollout_op['ground_truth_rollout'])}\n",
    "\n",
    "    # Add a leading axis, since Estimator's predict method insists that all\n",
    "    # tensors have a shared leading batch axis fo the same dims.\n",
    "    rollout_op = tree.map_structure(lambda x: x[tf.newaxis], rollout_op)\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        train_op=None,\n",
    "        loss=loss,\n",
    "        predictions=rollout_op,\n",
    "        eval_metric_ops=eval_ops)\n",
    "\n",
    "  return estimator_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1852857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_metadata(data_path):\n",
    "  with open(os.path.join(data_path, 'metadata.json'), 'rt') as fp:\n",
    "    return json.loads(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2739ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "  \"\"\"Train or evaluates the model.\"\"\"\n",
    "\n",
    "  if FLAGS.mode in ['train', 'eval']:\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        get_one_step_estimator_fn(FLAGS.data_path, FLAGS.noise_std),\n",
    "        model_dir=FLAGS.model_path)\n",
    "    if FLAGS.mode == 'train':\n",
    "      # Train all the way through.\n",
    "      estimator.train(\n",
    "          input_fn=get_input_fn(FLAGS.data_path, FLAGS.batch_size,\n",
    "                                mode='one_step_train', split='train'),\n",
    "          max_steps=FLAGS.num_steps)\n",
    "    else:\n",
    "      # One-step evaluation from checkpoint.\n",
    "      eval_metrics = estimator.evaluate(input_fn=get_input_fn(\n",
    "          FLAGS.data_path, FLAGS.batch_size,\n",
    "          mode='one_step', split=FLAGS.eval_split))\n",
    "      logging.info('Evaluation metrics:')\n",
    "      logging.info(eval_metrics)\n",
    "  elif FLAGS.mode == 'eval_rollout':\n",
    "    if not FLAGS.output_path:\n",
    "      raise ValueError('A rollout path must be provided.')\n",
    "    rollout_estimator = tf.estimator.Estimator(\n",
    "        get_rollout_estimator_fn(FLAGS.data_path, FLAGS.noise_std),\n",
    "        model_dir=FLAGS.model_path)\n",
    "\n",
    "    # Iterate through rollouts saving them one by one.\n",
    "    metadata = _read_metadata(FLAGS.data_path)\n",
    "    rollout_iterator = rollout_estimator.predict(\n",
    "        input_fn=get_input_fn(FLAGS.data_path, batch_size=1,\n",
    "                              mode='rollout', split=FLAGS.eval_split))\n",
    "\n",
    "    for example_index, example_rollout in enumerate(rollout_iterator):\n",
    "      example_rollout['metadata'] = metadata\n",
    "      filename = f'rollout_{FLAGS.eval_split}_{example_index}.pkl'\n",
    "      filename = os.path.join(FLAGS.output_path, filename)\n",
    "      logging.info('Saving: %s.', filename)\n",
    "      if not os.path.exists(FLAGS.output_path):\n",
    "        os.mkdir(FLAGS.output_path)\n",
    "      with open(filename, 'wb') as file:\n",
    "        pickle.dump(example_rollout, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f13afc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbbf6fd",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "500238a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "split='train'\n",
    "data_path = \"./tmp/datasets/Water\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4d16922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the metadata of the dataset.\n",
    "metadata = _read_metadata(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9593c82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bounds': [[0.1, 0.9], [0.1, 0.9]],\n",
       " 'sequence_length': 1000,\n",
       " 'default_connectivity_radius': 0.015,\n",
       " 'dim': 2,\n",
       " 'dt': 0.0025,\n",
       " 'vel_mean': [-4.906372733478189e-06, -0.0003581614249505887],\n",
       " 'vel_std': [0.0018492343327724738, 0.0018154400863548657],\n",
       " 'acc_mean': [-1.3758095862050814e-08, 1.114232425851392e-07],\n",
       " 'acc_std': [0.0001279824304831018, 0.0001388316140032424]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1def12ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tf.data.Dataset from the TFRecord.\n",
    "ds = tf.data.TFRecordDataset([os.path.join(data_path, f'{split}.tfrecord')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "117de3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function parse_serialized_simulation_example at 0x7f4980bd3e50> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Constant'\n",
      "WARNING: Entity <function parse_serialized_simulation_example at 0x7f4980bd3e50> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Constant'\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(functools.partial(\n",
    "    reading_utils.parse_serialized_simulation_example, metadata=metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "037ef927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function prepare_rollout_inputs at 0x7f4980bdbaf0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Constant'\n",
      "WARNING: Entity <function prepare_rollout_inputs at 0x7f4980bdbaf0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Constant'\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(prepare_rollout_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fd39fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ({particle_type: (?,), key: (), position: (?, 1000, 2), n_particles_per_example: (1,), is_trajectory: (1,)}, (?, 2)), types: ({particle_type: tf.int64, key: tf.int64, position: tf.float32, n_particles_per_example: tf.int32, is_trajectory: tf.bool}, tf.float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9784829b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'particle_type': TensorSpec(shape=(?,), dtype=tf.int64, name=None),\n",
       "  'key': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       "  'position': TensorSpec(shape=(?, 1000, 2), dtype=tf.float32, name=None),\n",
       "  'n_particles_per_example': TensorSpec(shape=(1,), dtype=tf.int32, name=None),\n",
       "  'is_trajectory': TensorSpec(shape=(1,), dtype=tf.bool, name=None)},\n",
       " TensorSpec(shape=(?, 2), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b41d8c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ({particle_type: (?,), key: (), position: (?, 1000, 2), n_particles_per_example: (1,), is_trajectory: (1,)}, (?, 2)), types: ({particle_type: tf.int64, key: tf.int64, position: tf.float32, n_particles_per_example: tf.int32, is_trajectory: tf.bool}, tf.float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29f8da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_iterator = tf.python_io.tf_record_iterator(path=os.path.join(data_path, f'{split}.tfrecord'))\n",
    "for string_record in record_iterator:\n",
    "    example = tf.train.Example()\n",
    "#     record = reading_utils.parse_serialized_simulation_example(string_record, metadata=metadata)\n",
    "    example.ParseFromString(string_record)\n",
    "#     print(example)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99e49287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.core.example.example_pb2.Example"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ddbd2b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1718497"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.ByteSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c84206db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"key\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"particle_type\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\\005\\000\\000\\000\\000\\000\\000\\000\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff0c0d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84676cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = dict(\n",
    "  latent_size=128,\n",
    "  mlp_hidden_size=128,\n",
    "  mlp_num_hidden_layers=2,\n",
    "  num_message_passing_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66fe05d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std = 6.7e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09c34cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = _get_simulator(model_kwargs, metadata,\n",
    "                           vel_noise_std=noise_std,\n",
    "                           acc_noise_std=noise_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45e9565a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<learning_to_simulate.learned_simulator.LearnedSimulator at 0x7fa0190496a0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b9f1e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<learning_to_simulate.graph_network.EncodeProcessDecode at 0x7fa018736eb0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulator._graph_network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad2017",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "648e6267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 02:29:48.939619: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-03 02:29:49.023540: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:49.023639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1666] Found device 0 with properties: \n",
      "name: NVIDIA RTX A4000 major: 8 minor: 6 memoryClockRate(GHz): 1.56\n",
      "pciBusID: 0000:15:00.0\n",
      "2022-04-03 02:29:49.024373: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:49.024416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1666] Found device 1 with properties: \n",
      "name: Quadro P2000 major: 6 minor: 1 memoryClockRate(GHz): 1.4805\n",
      "pciBusID: 0000:21:00.0\n",
      "2022-04-03 02:29:49.024457: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-04-03 02:29:49.042447: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-04-03 02:29:49.093751: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-03 02:29:49.094478: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-03 02:29:49.096795: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2022-04-03 02:29:49.099947: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-04-03 02:29:49.100274: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-04-03 02:29:49.100978: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:49.101506: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:49.101879: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:49.102218: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:49.102232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1794] Adding visible gpu devices: 0, 1\n",
      "2022-04-03 02:29:49.117035: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3599995000 Hz\n",
      "2022-04-03 02:29:49.119074: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x54e7d20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-04-03 02:29:49.119165: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-04-03 02:29:49.279380: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:49.304286: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:49.304565: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5442040 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-04-03 02:29:49.304592: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "2022-04-03 02:29:49.304600: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Quadro P2000, Compute Capability 6.1\n",
      "2022-04-03 02:29:49.305800: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:49.305871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1666] Found device 0 with properties: \n",
      "name: NVIDIA RTX A4000 major: 8 minor: 6 memoryClockRate(GHz): 1.56\n",
      "pciBusID: 0000:15:00.0\n",
      "2022-04-03 02:29:49.306432: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:49.306490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1666] Found device 1 with properties: \n",
      "name: Quadro P2000 major: 6 minor: 1 memoryClockRate(GHz): 1.4805\n",
      "pciBusID: 0000:21:00.0\n",
      "2022-04-03 02:29:49.306548: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-04-03 02:29:49.306698: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2022-04-03 02:29:49.306801: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-03 02:29:49.306875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-03 02:29:49.306901: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\n",
      "2022-04-03 02:29:49.306926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-04-03 02:29:49.306948: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-04-03 02:29:49.307918: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:49.308638: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:49.309598: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:49.310647: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:49.310713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1794] Adding visible gpu devices: 0, 1\n",
      "2022-04-03 02:29:49.310815: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-04-03 02:29:50.591861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-03 02:29:50.592012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212]      0 1 \n",
      "2022-04-03 02:29:50.592058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1225] 0:   N N \n",
      "2022-04-03 02:29:50.592065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1225] 1:   N N \n",
      "2022-04-03 02:29:50.593748: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:50.593821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-04-03 02:29:50.594706: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:50.594780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Could not identify NUMA node of platform GPU id 1, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-04-03 02:29:50.596780: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:50.596995: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-04-03 02:29:50.597107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1351] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13346 MB memory) -> physical GPU (device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:15:00.0, compute capability: 8.6)\n",
      "2022-04-03 02:29:50.600007: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1068] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-03 02:29:50.600141: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-04-03 02:29:50.600206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1351] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 3446 MB memory) -> physical GPU (device: 1, name: Quadro P2000, pci bus id: 0000:21:00.0, compute capability: 6.1)\n"
     ]
    }
   ],
   "source": [
    "from learning_to_simulate import graph_network\n",
    "\n",
    "# Classic way of exporting the graph using placeholders and an outside call to the model_fn\n",
    "with tf.Graph().as_default() as g:\n",
    "    # Placeholders\n",
    "#     features = tf.placeholder(tf.float32, x.shape)\n",
    "#     labels = tf.placeholder(tf.float32, y.shape)\n",
    "\n",
    "    # Creates the graph\n",
    "#     _ = model_fn(features, labels, None)\n",
    "    _graph_network = graph_network.EncodeProcessDecode(\n",
    "              output_size=metadata['dim'],\n",
    "              **model_kwargs)\n",
    "#               reducer: Reducer = tf.math.unsorted_segment_sum,\n",
    "#               name: str = \"EncodeProcessDecode\"):\n",
    "\n",
    "    # Export the graph to ./graph\n",
    "    with tf.Session() as sess:\n",
    "        train_writer = tf.summary.FileWriter('./logs', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "415722ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'cgi' has no attribute 'escape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtensorboard\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m--logdir logs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:2204\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2202\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2203\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2204\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/notebook.py:120\u001b[0m, in \u001b[0;36m_start_magic\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_start_magic\u001b[39m(line):\n\u001b[1;32m    119\u001b[0m   \u001b[38;5;124;03m\"\"\"Implementation of the `%tensorboard` line magic.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/notebook.py:158\u001b[0m, in \u001b[0;36mstart\u001b[0;34m(args_string)\u001b[0m\n\u001b[1;32m    155\u001b[0m start_result \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mstart(parsed_args)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(start_result, manager\u001b[38;5;241m.\u001b[39mStartLaunched):\n\u001b[0;32m--> 158\u001b[0m   \u001b[43m_display\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m      \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m      \u001b[49m\u001b[43mprint_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdisplay_handle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(start_result, manager\u001b[38;5;241m.\u001b[39mStartReused):\n\u001b[1;32m    165\u001b[0m   template \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    166\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReusing TensorBoard on port \u001b[39m\u001b[38;5;132;01m{port}\u001b[39;00m\u001b[38;5;124m (pid \u001b[39m\u001b[38;5;132;01m{pid}\u001b[39;00m\u001b[38;5;124m), started \u001b[39m\u001b[38;5;132;01m{delta}\u001b[39;00m\u001b[38;5;124m ago. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m!kill \u001b[39m\u001b[38;5;132;01m{pid}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to kill it.)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m   )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/notebook.py:316\u001b[0m, in \u001b[0;36m_display\u001b[0;34m(port, height, print_message, display_handle)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    311\u001b[0m fn \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    312\u001b[0m     _CONTEXT_COLAB: _display_colab,\n\u001b[1;32m    313\u001b[0m     _CONTEXT_IPYTHON: _display_ipython,\n\u001b[1;32m    314\u001b[0m     _CONTEXT_NONE: _display_cli,\n\u001b[1;32m    315\u001b[0m }[_get_context()]\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_handle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_handle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorboard/notebook.py:416\u001b[0m, in \u001b[0;36m_display_ipython\u001b[0;34m(port, height, display_handle)\u001b[0m\n\u001b[1;32m    402\u001b[0m frame_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard-frame-\u001b[39m\u001b[38;5;132;01m{:08x}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(random\u001b[38;5;241m.\u001b[39mgetrandbits(\u001b[38;5;241m64\u001b[39m))\n\u001b[1;32m    403\u001b[0m shell \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;124m    <iframe id=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mHTML_ID\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m width=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m height=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mHEIGHT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m frameborder=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;124m    </iframe>\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124m    </script>\u001b[39m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    415\u001b[0m replacements \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 416\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mHTML_ID\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mcgi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mescape\u001b[49m(frame_id, quote\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[1;32m    417\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mJSON_ID\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(frame_id)),\n\u001b[1;32m    418\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mPORT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m port),\n\u001b[1;32m    419\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mHEIGHT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m height),\n\u001b[1;32m    420\u001b[0m ]\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (k, v) \u001b[38;5;129;01min\u001b[39;00m replacements:\n\u001b[1;32m    422\u001b[0m   shell \u001b[38;5;241m=\u001b[39m shell\u001b[38;5;241m.\u001b[39mreplace(k, v)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cgi' has no attribute 'escape'"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07274c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
